{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import threading\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_small.csv', header=None)\n",
    "test_df = pd.read_csv('test_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>14</th>\n",
       "      <th>103</th>\n",
       "      <th>128</th>\n",
       "      <th>136</th>\n",
       "      <th>152</th>\n",
       "      <th>155</th>\n",
       "      <th>160</th>\n",
       "      <th>166</th>\n",
       "      <th>177</th>\n",
       "      <th>179</th>\n",
       "      <th>182</th>\n",
       "      <th>198</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>8.1267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>-6.5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>3.9267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>train_5</td>\n",
       "      <td>0</td>\n",
       "      <td>13.6380</td>\n",
       "      <td>19.8236</td>\n",
       "      <td>13.0031</td>\n",
       "      <td>-8.8256</td>\n",
       "      <td>19.5171</td>\n",
       "      <td>15.2331</td>\n",
       "      <td>24.2054</td>\n",
       "      <td>1.3847</td>\n",
       "      <td>9.2654</td>\n",
       "      <td>10.7687</td>\n",
       "      <td>7.1968</td>\n",
       "      <td>-3.6241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>train_6</td>\n",
       "      <td>0</td>\n",
       "      <td>14.1629</td>\n",
       "      <td>17.0682</td>\n",
       "      <td>12.1880</td>\n",
       "      <td>-13.2871</td>\n",
       "      <td>11.6169</td>\n",
       "      <td>13.1753</td>\n",
       "      <td>21.9814</td>\n",
       "      <td>6.1213</td>\n",
       "      <td>12.7942</td>\n",
       "      <td>8.1735</td>\n",
       "      <td>-8.1416</td>\n",
       "      <td>9.1104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70052</td>\n",
       "      <td>train_199983</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4175</td>\n",
       "      <td>9.4990</td>\n",
       "      <td>13.6724</td>\n",
       "      <td>-11.8286</td>\n",
       "      <td>15.6931</td>\n",
       "      <td>21.1326</td>\n",
       "      <td>22.5358</td>\n",
       "      <td>-2.9164</td>\n",
       "      <td>16.7099</td>\n",
       "      <td>12.3458</td>\n",
       "      <td>-0.1731</td>\n",
       "      <td>3.5126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70053</td>\n",
       "      <td>train_199987</td>\n",
       "      <td>0</td>\n",
       "      <td>14.2372</td>\n",
       "      <td>2.0500</td>\n",
       "      <td>13.3528</td>\n",
       "      <td>-13.6591</td>\n",
       "      <td>12.7498</td>\n",
       "      <td>14.6672</td>\n",
       "      <td>10.6501</td>\n",
       "      <td>-7.8169</td>\n",
       "      <td>9.6030</td>\n",
       "      <td>12.5392</td>\n",
       "      <td>-8.7072</td>\n",
       "      <td>-3.5687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70054</td>\n",
       "      <td>train_199993</td>\n",
       "      <td>0</td>\n",
       "      <td>14.3334</td>\n",
       "      <td>9.9091</td>\n",
       "      <td>14.1692</td>\n",
       "      <td>-0.8046</td>\n",
       "      <td>17.2302</td>\n",
       "      <td>17.8464</td>\n",
       "      <td>19.5502</td>\n",
       "      <td>-8.2583</td>\n",
       "      <td>12.3769</td>\n",
       "      <td>10.5019</td>\n",
       "      <td>-2.8878</td>\n",
       "      <td>6.7302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70055</td>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4131</td>\n",
       "      <td>9.5501</td>\n",
       "      <td>12.2389</td>\n",
       "      <td>-6.0942</td>\n",
       "      <td>18.7807</td>\n",
       "      <td>17.8085</td>\n",
       "      <td>15.3500</td>\n",
       "      <td>-9.7142</td>\n",
       "      <td>11.8542</td>\n",
       "      <td>11.1395</td>\n",
       "      <td>2.5058</td>\n",
       "      <td>-2.1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70056</td>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>14.2110</td>\n",
       "      <td>24.6626</td>\n",
       "      <td>13.5462</td>\n",
       "      <td>1.6146</td>\n",
       "      <td>18.8580</td>\n",
       "      <td>16.7224</td>\n",
       "      <td>15.9319</td>\n",
       "      <td>-12.6998</td>\n",
       "      <td>11.6621</td>\n",
       "      <td>8.1808</td>\n",
       "      <td>3.8748</td>\n",
       "      <td>6.7980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70057 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0    1        14       103      128      136      152  \\\n",
       "0           train_1    0  14.0239   8.4068  14.5697   1.9568  20.6294   \n",
       "1           train_2    0  14.1929  12.6317  14.1978   3.9358  14.3330   \n",
       "2           train_4    0  13.8481  12.3562  12.5815  -6.8304  20.1461   \n",
       "3           train_5    0  13.6380  19.8236  13.0031  -8.8256  19.5171   \n",
       "4           train_6    0  14.1629  17.0682  12.1880 -13.2871  11.6169   \n",
       "...             ...  ...      ...      ...      ...      ...      ...   \n",
       "70052  train_199983    0  14.4175   9.4990  13.6724 -11.8286  15.6931   \n",
       "70053  train_199987    0  14.2372   2.0500  13.3528 -13.6591  12.7498   \n",
       "70054  train_199993    0  14.3334   9.9091  14.1692  -0.8046  17.2302   \n",
       "70055  train_199996    0  14.4131   9.5501  12.2389  -6.0942  18.7807   \n",
       "70056  train_199998    0  14.2110  24.6626  13.5462   1.6146  18.8580   \n",
       "\n",
       "           155      160      166      177      179     182     198  \n",
       "0      16.7242  22.3949  -5.0121  10.4968  16.5721 -5.5937  8.1267  \n",
       "1      14.1479  33.8820  -1.0410  10.9202   8.8438 -4.2935 -6.5213  \n",
       "2      19.8234  27.0846   6.5769  16.3309  13.5261 -7.6938  3.9267  \n",
       "3      15.2331  24.2054   1.3847   9.2654  10.7687  7.1968 -3.6241  \n",
       "4      13.1753  21.9814   6.1213  12.7942   8.1735 -8.1416  9.1104  \n",
       "...        ...      ...      ...      ...      ...     ...     ...  \n",
       "70052  21.1326  22.5358  -2.9164  16.7099  12.3458 -0.1731  3.5126  \n",
       "70053  14.6672  10.6501  -7.8169   9.6030  12.5392 -8.7072 -3.5687  \n",
       "70054  17.8464  19.5502  -8.2583  12.3769  10.5019 -2.8878  6.7302  \n",
       "70055  17.8085  15.3500  -9.7142  11.8542  11.1395  2.5058 -2.1651  \n",
       "70056  16.7224  15.9319 -12.6998  11.6621   8.1808  3.8748  6.7980  \n",
       "\n",
       "[70057 rows x 14 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irregulars = [14, 103, 128, 136, 152, 155, 160, 166, 177, 179, 182, 198]\n",
    "train_df = train_df.iloc[:,[0, 1]+irregulars]\n",
    "test_df = test_df.iloc[:, np.array([1] + irregulars)-1]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       0.099733\n",
      "14     14.023154\n",
      "103    13.439792\n",
      "128    13.200582\n",
      "136    -4.815448\n",
      "152    16.817737\n",
      "155    16.717410\n",
      "160    17.932712\n",
      "166    -2.861032\n",
      "177    11.636009\n",
      "179    11.884891\n",
      "182    -2.745573\n",
      "198     2.298631\n",
      "dtype: float64\n",
      "var_12     14.024578\n",
      "var_101    13.375967\n",
      "var_126    13.204643\n",
      "var_134    -4.844257\n",
      "var_150    16.823070\n",
      "var_153    16.730608\n",
      "var_158    17.896491\n",
      "var_164    -2.876100\n",
      "var_175    11.629153\n",
      "var_177    11.887321\n",
      "var_180    -2.745725\n",
      "var_196     2.292337\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.mean())\n",
    "print(test_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 60000\n",
    "train_targets = train_df.iloc[:n_train,1]\n",
    "train_data = train_df.iloc[:n_train,2:]\n",
    "test_targets = train_df.iloc[n_train:,1]\n",
    "test_data = train_df.iloc[n_train:,2:]\n",
    "\n",
    "\n",
    "def auc2(m, train, test): \n",
    "    return (metrics.roc_auc_score(train_targets,m.predict(train)),\n",
    "                            metrics.roc_auc_score(test_targets,m.predict(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organise the data into numpy arrays so that it is handled correctly by imblearn\n",
    "nump_train_data = train_data.to_numpy()\n",
    "nump_train_data = nump_train_data.astype(float)\n",
    "nump_train_targets = train_targets.to_numpy()\n",
    "nump_train_targets = nump_train_targets.astype(int)\n",
    "\n",
    "nump_test_data = test_data.to_numpy()\n",
    "nump_test_data = nump_test_data.astype(float)\n",
    "nump_test_targets = test_targets.to_numpy()\n",
    "nump_test_targets = nump_test_targets.astype(int)\n",
    "\n",
    "rus = imblearn.under_sampling.RandomUnderSampler(sampling_strategy='auto',return_indices=True)\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(nump_train_data, nump_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  54 | elapsed:   28.4s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.5, max_depth=1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=300, n_jobs=-1, num_leaves=2, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = lgb.LGBMRegressor(silent=False)\n",
    "param_dist = {\"max_depth\": [1,2, 3],\n",
    "              \"learning_rate\" : [0.1,0.5,1],\n",
    "              # \"num_leaves\": [10,15,20],\n",
    "              \"num_leaves\": [2,3],\n",
    "              \"n_estimators\": [300]\n",
    "             }\n",
    "grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "grid_search.fit(train_data,train_targets)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.5, max_depth=10,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=300, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.941760012507842, 0.8508816764850232)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2(grid_search.best_estimator_, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best predictors\n",
    "500 rows 150 estimators (12s) : depth 3; rate 0.5; 10 leaves;\n",
    "\n",
    "1000 rows 200 estimators (19s) : depth 2; rate 0.1; 10 leaves; 0.7361517358288425 auc (1 with training set)\n",
    "\n",
    "10000 rows 300 estimators (1min1s) : depth 1; rate 0.5; 10 leaves; 0.8508816764850232 auc (0.941760012507842 with training set)\n",
    "\n",
    "Does number of leaves impact ? NO default 31\n",
    "\n",
    "60000 rows; 300 estiators; depth 1; rate 0.5; 10 leaves : (0.8986034603281355, 0.8795559277631042) (train then test)\n",
    "60000 rows; 1000 estiators; depth 1; rate 0.5; 10 leaves : (0.9183773853908943, 0.8924150185745882) (train then test)\n",
    "#### Shows that overfitting is not good ? Or just harder to have high auc with bigger amount of data.\n",
    "60000 rows; 10000 estiators; depth 1; rate 0.5; 10 leaves : (0.9433394334525005, 0.8876156885481759) (train then test)\n",
    "#### Now it seems clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.802568674087524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9183773853908943, 0.8924150185745882)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lggood = lgb.LGBMRegressor(silent=False, max_depth=1, learning_rate=0.5, n_estimators=1000)\n",
    "start = time.time()\n",
    "lggood.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "auc2(lggood, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              importance_type='split', learning_rate=0.5, max_depth=1,\n",
      "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "              n_estimators=1000, n_jobs=-1, num_leaves=31, objective=None,\n",
      "              random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "              scale_pos_weight=100, silent=False, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0)\n"
     ]
    }
   ],
   "source": [
    "print(lggood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate 0.5 1000 estim : (0.9183773853908943, 0.8924150185745882) in 6.60472559928894s\n",
    "rate 0.3 1000 estim : (0.9158483146682737, 0.8915313835384053) in 5.39690375328064s\n",
    "0.3 800 5.174034118652344\n",
    "(0.91017955230423, 0.8890010522021087)\n",
    "0.5 1500 8.940380811691284\n",
    "(0.9225535053779779, 0.892192660353454)\n",
    "\n",
    "#### Overfitting : \n",
    "rate 0.5 1000 estim depth 2 : (0.9736699095251268, 0.8562574888874572) 9.129353523254395s\n",
    "rate 0.5 300 estim depth 2 : (0.9332266802884277, 0.8697272863922353) kinda 3.4s\n",
    "rate 1 200 est depth 5 : (0.9997954560831425, 0.7009104769267109) in 5.8162806034088135\n",
    "rate 1 200 est depth 5 : (1.0, 0.6860658377890871) in 26.31138586997986s\n",
    "\n",
    "\n",
    "#### Mmmmmh\n",
    "rate 0.1 200 est depth 5 : (0.9999726974419348, 0.8615919388434367) in 23.673893928527832s\n",
    "0.03 1000 est : 25.953459978103638s (0.9924414078232998, 0.8748606369043784)\n",
    "0.01 : 25.04612970352173 (0.9542057987408105, 0.8484477871545448)\n",
    "0.01 3000 est : (0.9925341998563096, 0.8746814404432133) 78.72248482704163s (meh)\n",
    "\n",
    "#### To few estimators in this case (?):\n",
    "rate 0.1 300 estim depth 2 : (0.8918921360973638, 0.8568826902016364) 3.3066024780273438s\n",
    "rate 0.1 1000 estim depth 2 : (0.9356584753554953, 0.8837796602890337) 8.71259069442749s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.418010711669922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9144309306569556, 0.888129979170693)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgrus = lgb.LGBMRegressor(silent=False, max_depth=1, learning_rate=0.1, n_estimators=3000)\n",
    "start = time.time()\n",
    "lgrus.fit(X_rus,y_rus)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "auc2(lgrus, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing data\n",
    "scale_pos_weight useless\n",
    "with RUS:\n",
    "rate 0.5 1000 estim : 1.704514980316162s : (0.9156733819245538, 0.8793312074556034)\n",
    "rate 0.1 3000 estim : (0.9144309306569556, 0.888129979170693) 4.4809534549713135s\n",
    "rate 0.1 4000 estim : (0.9172283755513937, 0.887266314501063)\n",
    "rate 0.08 5000 estim : 7.250913619995117s (0.9161398215981195, 0.8879848182267172)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bag_pred:\n",
    "    def __init__(self, predictors=[], weights=None, scatter=True):\n",
    "        self.predictors = predictors\n",
    "        if weights==None:\n",
    "            self.weights = list(np.ones(len(predictors)))\n",
    "        else:\n",
    "            self.weights = list(weights)\n",
    "        self.scatter = scatter\n",
    "            \n",
    "    def predict(self, Y):\n",
    "        predictions = [pred.predict(Y) for pred in self.predictors]\n",
    "        weights = np.array(self.weights)/sum(self.weights)\n",
    "        total = weights[0]*np.array(self.predictors[0].predict(Y))\n",
    "        total.shape = len(total)\n",
    "        for pred, weight in zip(predictions, weights[1:]):\n",
    "            prediction = weight*np.array(pred)\n",
    "            prediction.shape = len(prediction)\n",
    "            total += prediction\n",
    "        return total\n",
    "    \n",
    "    \n",
    "    def predict1(self, Y):\n",
    "        \n",
    "        def Lambda(predictor,Y,predicts,i):\n",
    "            predicts[i] = predictor.predict(Y)\n",
    "        \n",
    "        threads = []\n",
    "        predictions = 5*[0]\n",
    "        for i, pred in enumerate(self.predictors):\n",
    "            threads.append(threading.Thread(target=Lambda, args=(pred,Y,predictions,i)))\n",
    "            threads[i].start()\n",
    "        for i, _ in enumerate(self.predictors):\n",
    "            threads[i].join()\n",
    "        \n",
    "        weights = np.array(self.weights)/sum(self.weights)\n",
    "        total = weights[0]*np.array(predictions[0])\n",
    "        total.shape = len(total)\n",
    "        for pred, weight in zip(predictions, weights[1:]):\n",
    "            prediction = weight*np.array(pred)\n",
    "            prediction.shape = len(prediction)\n",
    "            total += prediction\n",
    "        return total\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if(self.scatter):\n",
    "            factor = int(X/len(self.predictors))\n",
    "            for i, pred in in self.predictors:\n",
    "                low, high = factor*i, factor*(i+1)\n",
    "                yi = train_df.iloc[low:high,1]\n",
    "                Xi = train_df.iloc[low:high,2:]\n",
    "                pred.fit(Xi, yi)\n",
    "        else:\n",
    "            for pred in self.predictors:\n",
    "                pred.fit(X,y)\n",
    "\n",
    "            \n",
    "    def append(self, pred, weight=1):\n",
    "        self.predictors.append(pred)\n",
    "        self.weights.append(weight)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.05464506149292\n",
      "(0.9101142647942649, 0.8183159397882712)\n",
      "23.003049850463867\n"
     ]
    }
   ],
   "source": [
    "# Let's BAG\n",
    "Bag = Bag_pred()\n",
    "start = time.time()\n",
    "for i in range(6):\n",
    "    low, high = 10000*i, 10000*(i+1)\n",
    "    train_targetsi = train_df.iloc[low:high,1]\n",
    "    train_datai = train_df.iloc[low:high,2:]\n",
    "    \n",
    "    def auci(m, train, test): \n",
    "        return (metrics.roc_auc_score(train_targetsi,m.predict(train)),\n",
    "                metrics.roc_auc_score(test_targets,m.predict(test)))\n",
    "    \n",
    "    lgover = lgb.LGBMRegressor(silent=False, max_depth=5, learning_rate=0.035, n_estimators=200)\n",
    "    #lgover.fit(train_datai, train_targetsi)\n",
    "    #print(i, auci(lgover, train_datai, test_data))\n",
    "    Bag.append(lgover)\n",
    "Bag.fit(train_data, train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "start = time.time()\n",
    "print(auc2(Bag, train_data, test_data))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No reinit of the bag ....\n",
    "6 depth 5 r 0.035 n 200 (0.9345731839134271, 0.8709206768451114) = with n=400\n",
    "6 depth 3 r 0.035 n 200 (0.9265064602771006, 0.8644267645858833)\n",
    "6 depth 3 r 0.035 n 400 (0.9235606891946174, 0.8654291481457622) in 13s\n",
    "6 depth 5 r 0.035 n 100 ~0.86\n",
    "6 depth 5 r 0.035 n 50 (0.9311799920441334, 0.8681047477935966) in 6 + 43\n",
    "6 depth 5 r 0.035 n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.354048490524292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9127614850298509, 0.8919345487341365)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbr = cb.CatBoostRegressor(silent=True, depth=1, learning_rate=0.3, iterations=1000)\n",
    "# l2 leaf reg ??\n",
    "start = time.time()\n",
    "cbr.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "auc2(cbr, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate 0.5 : (0.917555524639378, 0.8912588846657648) in 16.263858318328857s\n",
    "rate 0.3 : (0.9127614850298509, 0.8919345487341365) in 14.216562747955322s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAMERAND\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\LAMERAND\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.60874819755554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9068527447700975, 0.8810668040971461)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbr = xgb.XGBRegressor(silent=False, max_depth=1, learning_rate=0.5, n_estimators=300, objective='binary:logistic')\n",
    "# min_child_weight ??\n",
    "start = time.time()\n",
    "xgbr.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "auc2(xgbr, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.5, max_delta_step=0,\n",
       "             max_depth=1, min_child_weight=1, missing=None, n_estimators=500,\n",
       "             n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "             seed=None, silent=False, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 estimators : (0.9068527447700975, 0.8810668040971461) in 76.34813570976257\n",
    "500 estimators : (0.9178513085660571, 0.8891569499022955) in 137.13004994392395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is not very Parallel with stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   38.2s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  21 | elapsed:  1.2min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  21 | elapsed:  1.2min remaining:   23.2s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.87924218177795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.30000000000000004,\n",
       "              max_depth=1, min_child_samples=20, min_child_weight=0.001,\n",
       "              min_split_gain=0.0, n_estimators=1000, n_jobs=-1, num_leaves=31,\n",
       "              objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=False, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = lgb.LGBMRegressor(silent=False)\n",
    "param_dist = {\"max_depth\": [1],\n",
    "              \"learning_rate\" : np.arange(0.2,0.8,0.1),\n",
    "              # \"num_leaves\": [10,15,20],\n",
    "              #\"num_leaves\": [2,3],\n",
    "              \"n_estimators\": [1000]\n",
    "             }\n",
    "grid_search = GridSearchCV(lgr, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "start = time.time()\n",
    "grid_search.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(grid_search.best_estimator_.learning_rate)\n",
    "print(auc2(grid_search.best_estimator_, train_data, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate does not seem to impact much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:   52.2s remaining:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  1.1min remaining:   58.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  1.8min remaining:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.06517004966736\n",
      "800\n",
      "(0.9158483146682737, 0.8915313835384053)\n"
     ]
    }
   ],
   "source": [
    "lgr = lgb.LGBMRegressor(silent=False)\n",
    "param_dist = {\"max_depth\": [1],\n",
    "              \"learning_rate\" : [0.5],\n",
    "              # \"num_leaves\": [10,15,20],\n",
    "              #\"num_leaves\": [2,3],\n",
    "              \"n_estimators\": [800, 1000, 1500, 2000, 3000]\n",
    "             }\n",
    "grid_search = GridSearchCV(lgr, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "start = time.time()\n",
    "grid_search.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(grid_search.best_estimator_.n_estimators)\n",
    "print(auc2(grid_search.best_estimator_, train_data, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neither a precise number estims ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingCVClassifier(classifiers=[LGBMRegressor(boosting_type='gbdt',\n",
       "                                                class_weight=None,\n",
       "                                                colsample_bytree=1.0,\n",
       "                                                importance_type='split',\n",
       "                                                learning_rate=0.5, max_depth=1,\n",
       "                                                min_child_samples=20,\n",
       "                                                min_child_weight=0.001,\n",
       "                                                min_split_gain=0.0,\n",
       "                                                n_estimators=1000, n_jobs=-1,\n",
       "                                                num_leaves=31, objective=None,\n",
       "                                                random_state=None,\n",
       "                                                reg_alpha=0.0, reg_lambda=0.0,\n",
       "                                                silent=False, subsampl...\n",
       "                                                           min_impurity_decrease=0.0,\n",
       "                                                           min_impurity_split=None,\n",
       "                                                           min_samples_leaf=1,\n",
       "                                                           min_samples_split=2,\n",
       "                                                           min_weight_fraction_leaf=0.0,\n",
       "                                                           presort=False,\n",
       "                                                           random_state=None,\n",
       "                                                           splitter='best'),\n",
       "                     n_jobs=None, pre_dispatch='2*n_jobs', random_state=785,\n",
       "                     shuffle=True, store_train_meta_features=False,\n",
       "                     stratify=True, use_clones=True,\n",
       "                     use_features_in_secondary=False, use_probas=False,\n",
       "                     verbose=0)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "\n",
    "meta = DecisionTreeRegressor()\n",
    "sclf = StackingCVClassifier(classifiers=[lggood, xgbr, cbr, lgrus],\n",
    "                            meta_classifier=meta,\n",
    "                            random_state=785)\n",
    "sclf.fit(train_data,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.meta_classifier.fit(train_data,train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.tree._tree.Tree at 0x22401cb1d50>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.meta_classifier.tree_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5497382915673518)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2(sclf.meta_classifier, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = [lggood, xgbr, cbr, lgrus, Bag]\n",
    "mat = []\n",
    "for model in stack:\n",
    "    mat.append(model.predict(train_data))\n",
    "meta_df = pd.DataFrame(np.transpose(np.array(mat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAMERAND\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "meta = LogisticRegression()\n",
    "meta.fit(meta_df, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = [lggood, xgbr, cbr, lgrus, Bag]\n",
    "mat = []\n",
    "for model in stack:\n",
    "    mat.append(model.predict(test_data))\n",
    "meta_df_test = pd.DataFrame(np.transpose(np.array(mat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4\n",
      "0      0.360781  0.258471  0.314487  0.862897  0.151870\n",
      "1     -0.052274  0.014589 -0.052279  0.095015  0.054187\n",
      "2      0.151852  0.085439  0.153031  0.477342  0.081380\n",
      "3      0.056884  0.031288  0.079956  0.330975  0.080134\n",
      "4      0.147655  0.227023  0.153765  0.544773  0.102666\n",
      "...         ...       ...       ...       ...       ...\n",
      "59995  0.524123  0.547292  0.472190  0.889156  0.346260\n",
      "59996 -0.082391  0.010415 -0.068166 -0.015968  0.042841\n",
      "59997 -0.035736  0.015842 -0.018454  0.090210  0.046670\n",
      "59998  0.694818  0.946699  0.624575  1.085728  0.376242\n",
      "59999  0.143685  0.079833  0.153201  0.612880  0.153800\n",
      "\n",
      "[60000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7453738173688175, 0.6693325495501299)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2(meta, meta_df, meta_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 3 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 4] <= 0.19285798817873 else to node 2.\n",
      "\tnode=1 leaf node.\n",
      "\tnode=2 leaf node.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-d29679c690ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# through the node j.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0mnode_indicator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m# Similarly, we can also have the leaves ids reached by each sample.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "estimator = meta\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()\n",
    "\n",
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "node_indicator = estimator.decision_path(X_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = estimator.apply(X_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             X_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))\n",
    "\n",
    "# For a group of samples, we have the following common node.\n",
    "sample_ids = [0, 1]\n",
    "common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                len(sample_ids))\n",
    "\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "      % (sample_ids, common_node_id))\n",
    "print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.36787152290344\n",
      "[0.88433053 0.88367097 0.89130639 0.88945922 0.88313811 0.89352368\n",
      " 0.89327486]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "scores = model_selection.cross_val_score(lggood, train_data, train_targets, cv=7, scoring='roc_auc', verbose=Tue)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8883862521165146\n"
     ]
    }
   ],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
