{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: so far the notebook is read-only. You need to download __[flights.csv](https://www.kaggle.com/usdot/flight-delays/data)__ before running the notebook. Also run one of XGBoost, LightGBM, or CatBoost while commenting out the other two.\n",
    " \n",
    " Notebook source: Toward Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import threading\n",
    "%matplotlib inline\n",
    "\n",
    "train_df = pd.read_csv('train_small.csv', header=None)\n",
    "test_df = pd.read_csv('test_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 60000\n",
    "train_targets = train_df.iloc[:n_train,1]\n",
    "train_data = train_df.iloc[:n_train,2:]\n",
    "test_targets = train_df.iloc[n_train:,1]\n",
    "test_data = train_df.iloc[n_train:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  54 | elapsed:   28.4s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:   30.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.5, max_depth=1,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=300, n_jobs=-1, num_leaves=2, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc2(m, train, test): \n",
    "    return (metrics.roc_auc_score(train_targets,m.predict(train)),\n",
    "                            metrics.roc_auc_score(test_targets,m.predict(test)))\n",
    "\n",
    "lg = lgb.LGBMRegressor(silent=False)\n",
    "param_dist = {\"max_depth\": [1,2, 3],\n",
    "              \"learning_rate\" : [0.1,0.5,1],\n",
    "              # \"num_leaves\": [10,15,20],\n",
    "              \"num_leaves\": [2,3],\n",
    "              \"n_estimators\": [300]\n",
    "             }\n",
    "grid_search = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 3, scoring=\"roc_auc\", verbose=5)\n",
    "grid_search.fit(train_data,train_targets)\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best predictors\n",
    "500 rows 150 estimators (12s) : depth 3; rate 0.5; 10 leaves;\n",
    "\n",
    "1000 rows 200 estimators (19s) : depth 2; rate 0.1; 10 leaves; 0.7361517358288425 auc (1 with training set)\n",
    "\n",
    "10000 rows 300 estimators (1min1s) : depth 1; rate 0.5; 10 leaves; 0.8508816764850232 auc (0.941760012507842 with training set)\n",
    "\n",
    "Does number of leaves impact ? NO default 31\n",
    "\n",
    "60000 rows; 300 estiators; depth 1; rate 0.5; 10 leaves : (0.8986034603281355, 0.8795559277631042) (train then test)\n",
    "60000 rows; 1000 estiators; depth 1; rate 0.5; 10 leaves : (0.9183773853908943, 0.8924150185745882) (train then test)\n",
    "#### Shows that overfitting is not good ? Or just harder to have high auc with bigger amount of data.\n",
    "60000 rows; 10000 estiators; depth 1; rate 0.5; 10 leaves : (0.9433394334525005, 0.8876156885481759) (train then test)\n",
    "#### Now it seems clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.65645384788513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9433394334525005, 0.8876156885481759)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "lggood = lgb.LGBMRegressor(silent=False, max_depth=1, learning_rate=0.5, n_estimators=10000)\n",
    "lggood.fit(train_data,train_targets)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "auc2(lggood, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.5, max_depth=10,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=300, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.941760012507842, 0.8508816764850232)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc2(grid_search.best_estimator_, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e5473d05be8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m grid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n\u001b[0;32m     15\u001b[0m                                    verbose=10, n_jobs=-1)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "\n",
    "def auc(m, train, test): \n",
    "    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n",
    "                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n",
    "\n",
    "# Parameter Tuning\n",
    "model = xgb.XGBRegressor()\n",
    "param_dist = {\"max_depth\": [10,30,50],\n",
    "              \"min_child_weight\" : [1,3,6],\n",
    "              \"n_estimators\": [100],\n",
    "              \"learning_rate\": [0.05, 0.1,0.16],}\n",
    "grid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n",
    "                                   verbose=10, n_jobs=-1)\n",
    "grid_search.fit(train, y_train)\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "print(model)\n",
    "\n",
    "model = xgb.XGBClassifier(max_depth=50, min_child_weight=1,  n_estimators=200,\\\n",
    "                          n_jobs=-1 , verbose=1,learning_rate=0.16)\n",
    "model.fit(train,y_train)\n",
    "\n",
    "auc(model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(train, label=y_train)\n",
    "params = {\"max_depth\": 50, \"learning_rate\" : 0.1, \"num_leaves\": 900,  \"n_estimators\": 300}\n",
    "\n",
    "# Without Categorical Features\n",
    "model2 = lgb.train(params, d_train)\n",
    "auc2(model2, train, test)\n",
    "\n",
    "#With Catgeorical Features\n",
    "cate_features_name = [\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"DESTINATION_AIRPORT\",\n",
    "                 \"ORIGIN_AIRPORT\"]\n",
    "model2 = lgb.train(params, d_train, categorical_feature = cate_features_name)\n",
    "auc2(model2, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "cat_features_index = [0,1,2,3,4,5,6]\n",
    "\n",
    "def auc(m, train, test): \n",
    "    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n",
    "                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n",
    "\n",
    "params = {'depth': [4, 7, 10],\n",
    "          'learning_rate' : [0.03, 0.1, 0.15],\n",
    "         'l2_leaf_reg': [1,4,9],\n",
    "         'iterations': [300]}\n",
    "cb = cb.CatBoostClassifier()\n",
    "cb_model = GridSearchCV(cb, params, scoring=\"roc_auc\", cv = 3)\n",
    "cb_model.fit(train, y_train)\n",
    "\n",
    "With Categorical features\n",
    "clf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
    "clf.fit(train,y_train)\n",
    "auc(clf, train, test)\n",
    "\n",
    "With Categorical features\n",
    "clf = cb.CatBoostClassifier(eval_metric=\"AUC\",one_hot_max_size=31, \\\n",
    "                            depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\n",
    "clf.fit(train,y_train, cat_features= cat_features_index)\n",
    "auc(clf, train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
